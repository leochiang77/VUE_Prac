安裝

Yolov7網址 : https://github.com/WongKinYiu/yolov7
預訓練模型下載網址 : https://github.com/WongKinYiu/yolov7/releases
	yolov7-tiny.pt (輕量，適合在樹莓派)
	yolov7_training.pt (更精準，適合在電腦)

pip install –r requirements.txt

使用GPU版需要有安裝CUDA和CUDNN
教學: https://medium.com/ching-i/win10-%E5%AE%89%E8%A3%9D-cuda-cudnn-%E6%95%99%E5%AD%B8-c617b3b76deb

-------------------------------------------

標記

https://github.com/HumanSignal/labelImg/releases

-------------------------------------------

資料處理

增量
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import cv2
import os 
import shutil
import numpy as np

def image_process(img, alpha, beta):
        img = np.float32(img)
        img = cv2.multiply(img, np.array([alpha]))
        img = cv2.add(img, beta)
        img = np.clip(img, 0, 255)
        img = np.uint8(img)
        img = cv2.fastNlMeansDenoisingColored(img, None, 3,3,7,21)
        return img

def create_txtFile(filename):
    with open(filename, 'w+') as f:
        pass
def write_newTxt(filename, word):
    with open(filename, 'a') as r:
        r.write(word[0]+" ")
        r.write(str(round(word[1],6))+" ")
        r.write(str(round(word[2],6))+" ")
        r.write(str(round(word[3],6))+" ")
        r.write(str(round(word[4],6))+"\n")

min_alpha = 0.5
max_alpha = 1.5
min_beta = 0
max_beta = 30
alpha = 0
beta = 0

allFileList = os.listdir("./label/")
for file in allFileList:
    alpha = min_alpha
    beta = min_beta
    
    if file.endswith('.jpg'):
        print(file)
        txt_file = file.replace(".jpg", ".txt")
        print(txt_file)
        image = cv2.imread("./label/"+file)
        
        while alpha <= max_alpha:
            while beta <= max_beta:
                img = image_process(image, alpha, beta)
                img2 = cv2.flip(img, 0)
                img3 = cv2.flip(img, 1)
                img4 = cv2.flip(img2, 1)
                cv2.imwrite('./augmentation/'+str(int(10*alpha))+"_"+str(beta)+"a"+file, img)
                cv2.imwrite('./augmentation/'+str(int(10*alpha))+"_"+str(beta)+"b"+file, img2)
                cv2.imwrite('./augmentation/'+str(int(10*alpha))+"_"+str(beta)+"c"+file, img3)
                cv2.imwrite('./augmentation/'+str(int(10*alpha))+"_"+str(beta)+"d"+file, img4)
                
                create_txtFile('./augmentation/'+str(int(10*alpha))+"_"+str(beta)+"a"+txt_file)
                create_txtFile('./augmentation/'+str(int(10*alpha))+"_"+str(beta)+"b"+txt_file)
                create_txtFile('./augmentation/'+str(int(10*alpha))+"_"+str(beta)+"c"+txt_file)
                create_txtFile('./augmentation/'+str(int(10*alpha))+"_"+str(beta)+"d"+txt_file)
                with open("./label/"+txt_file) as f:
                    for line in f.readlines():
                        try:
                            word = line.split(" ")
                            word_a = [word[0], float(word[1]), float(word[2]), float(word[3]), float(word[4])]
                            word_b = [word[0], float(word[1]), 1-float(word[2]), float(word[3]), float(word[4])]
                            word_c = [word[0], 1-float(word[1]), float(word[2]), float(word[3]), float(word[4])]
                            word_d = [word[0], 1-float(word[1]), 1-float(word[2]), float(word[3]), float(word[4])]
                            write_newTxt('./augmentation/'+str(int(10*alpha))+"_"+str(beta)+"a"+txt_file, word_a)
                            write_newTxt('./augmentation/'+str(int(10*alpha))+"_"+str(beta)+"b"+txt_file, word_b)
                            write_newTxt('./augmentation/'+str(int(10*alpha))+"_"+str(beta)+"c"+txt_file, word_c)
                            write_newTxt('./augmentation/'+str(int(10*alpha))+"_"+str(beta)+"d"+txt_file, word_d)
                        except:
                            pass
                beta+=15
            beta = min_beta
            alpha += 0.5

---------------------------
資料處理

拆分
# -*- coding: utf-8 -*-
import os
import random
import shutil

# 資料夾路徑
folder_path = "./augmentation"

# 存放訓練集的資料夾路徑
train_folder_path = "./augmentation/train"

# 存放測試集的資料夾路徑
test_folder_path = "./augmentation/val"

# 訓練集的比例（0~1之間的小數）
train_ratio = 0.8

# 檢查目標資料夾是否存在，如果不存在則建立
if not os.path.exists(train_folder_path):
    os.makedirs(train_folder_path)
    os.makedirs(os.path.join(train_folder_path, "images"))
    os.makedirs(os.path.join(train_folder_path, "labels"))
if not os.path.exists(test_folder_path):
    os.makedirs(test_folder_path)
    os.makedirs(os.path.join(test_folder_path, "images"))
    os.makedirs(os.path.join(test_folder_path, "labels"))

# 列出所有圖片檔案
image_files = [f for f in os.listdir(folder_path) if f.endswith(".jpg")]  # 根據你的圖片副檔名修改

# 隨機打亂檔案順序
random.shuffle(image_files)

# 計算訓練集的檔案數
train_count = int(len(image_files) * train_ratio)

# 分割訓練集和測試集
train_files = image_files[:train_count]
test_files = image_files[train_count:]

# 搬移圖片檔案和相應的標記檔案到訓練集資料夾的images和labels資料夾
for train_file in train_files:
    image_file_path = os.path.join(folder_path, train_file)
    label_file_path = os.path.join(folder_path, os.path.splitext(train_file)[0] + ".txt")
    shutil.move(image_file_path, os.path.join(train_folder_path, "images", train_file))
    shutil.move(label_file_path, os.path.join(train_folder_path, "labels", os.path.splitext(train_file)[0] + ".txt"))
    print(f"移動檔案到訓練集: {train_file}")

# 搬移圖片檔案和相應的標記檔案到測試集資料夾的images和labels資料夾
for test_file in test_files:
    image_file_path = os.path.join(folder_path, test_file)
    label_file_path = os.path.join(folder_path, os.path.splitext(test_file)[0] + ".txt")
    shutil.move(image_file_path, os.path.join(test_folder_path, "images", test_file))
    shutil.move(label_file_path, os.path.join(test_folder_path, "labels", os.path.splitext(test_file)[0] + ".txt"))
    print(f"移動檔案到測試集: {test_file}")
	
--------------------------------------------

資料處理

前處理

1.循著Yolo資料夾的路徑進來 [Yolov7 >> cfg >> training]
2.進到training資料夾後，複製一份官方的yaml檔案，並修改為喜歡的檔名
3.根據classes.txt裡有多少類別，將新檔案內的nc數值修改

1.循著Yolo資料夾的路徑進來 [Yolov7 >> data]
2.進到data資料夾後，創建訓練集和驗證集的資料夾，並分別再建立放圖片和標記檔的子資料夾
3.接著，複製coco.yaml檔案，並修改為喜歡的檔名
4.根據classes.txt裡有多少類別，將新檔案內的nc數值修改
5.將train和val路徑改為存放資料的位置
6.將classes.txt裡的類別填到class names中，格式為names: [‘類別1’, ‘類別2’…]

1.確認資料集有放好
2.確認參數檔案設定正確
3.Labels.cache刪除

------------------------------------------------
訓練

開啟終端機，進入yolov7的資料夾跟環境

python train.py --workers 8 --device 0 --batch-size 32 --epochs 300 --data data/custom_M11.yaml --cfg cfg/training/yolov7-tiny-custom-M11.yaml --weights 'yolov7-tiny.pt' --name yolov7-tiny-custom-M11 --hyp data/hyp.scratch.tiny.yaml

--workers ：指定用於數據加載的工作進程數量。
--device ：指定 GPU 設備的ID（如果系統上有多個GPU）。
--batch-size ：指定每個訓練步驟中的圖像批次大小。
--epochs ：指定訓練的總過程數，每個epoch代表對數據集的一次完整遍歷。
--data ：指定數據集的配置文件位置。
--cfg ：指定模型的配置文件位置。
--weights ：指定用於初始化模型權重的預設文件的位置。
--name ：指定訓練過程中保存模型的名稱。
--hyp ：指定超參數配置文件的位置

--------------------------------------------------
測試

訓練產生的權重(模型)會自動存放在 :
Yolov7 >> runs >> train >> 前面的name參數資料夾 >> weights

1.在Yolov7內創建一個test資料夾(Yolov7 >> test )
2.將要測試的圖片放入test資料夾內
3.在終端機中執行測試指令
python detect_custom.py --device 1 --weights runs/train/yolov7-tiny-custom-M11/weights/best.pt
4.結果會顯示在test資料夾內

detect_custom.py
# coding: utf-8

# In[ ]:


import os
import argparse
import time
from pathlib import Path
import time
import cv2
import torch
import torch.backends.cudnn as cudnn
from numpy import random

from models.experimental import attempt_load
from utils.datasets import LoadStreams, LoadImages
from utils.general import check_img_size, check_requirements, check_imshow, non_max_suppression, apply_classifier,     scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path
from utils.plots import plot_one_box
from utils.torch_utils import select_device, load_classifier, time_synchronized, TracedModel


class detect:
    def __init__(self):
        self.weights, self.view_img, self.save_txt, self.imgsz, self.trace = opt.weights, opt.view_img, opt.save_txt, opt.img_size, not opt.no_trace
        # Directories
        self.save_dir = Path(increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok))  # increment run
        (self.save_dir / 'labels' if self.save_txt else self.save_dir).mkdir(parents=True, exist_ok=True)  # make dir

        # Initialize
        set_logging()
        self.device = select_device(opt.device)
        self.half = self.device.type != 'cpu'  # half precision only supported on CUDA

        # Load model
        self.model = attempt_load(self.weights, map_location=self.device)  # load FP32 model
        self.stride = int(self.model.stride.max())  # model stride
        self.imgsz = check_img_size(self.imgsz, s=self.stride)  # check img_size

        if self.trace:
            self.model = TracedModel(self.model, self.device, opt.img_size)

        if self.half:
            self.model.half()  # to FP16
            
    def inference(self, image):
        self.source = image
        self.save_img=False
        self.save_img = not opt.nosave and not self.source.endswith('.txt')  # save inference images
        self.webcam = self.source.isnumeric() or self.source.endswith('.txt') or self.source.lower().startswith(
            ('rtsp://', 'rtmp://', 'http://', 'https://'))
        # Set Dataloader
        vid_path, vid_writer = None, None
        if self.webcam:
            self.view_img = check_imshow()
            cudnn.benchmark = True  # set True to speed up constant image size inference
            dataset = LoadStreams(self.source, img_size=self.imgsz, stride=self.stride)
        else:
            dataset = LoadImages(self.source, img_size=self.imgsz, stride=self.stride)

        # Get names and colors
        names = self.model.module.names if hasattr(self.model, 'module') else self.model.names
        colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]

        # Run inference
        if self.device.type != 'cpu':
            self.model(torch.zeros(1, 3, self.imgsz, self.imgsz).to(self.device).type_as(next(self.model.parameters())))  # run once
        old_img_w = old_img_h = self.imgsz
        old_img_b = 1

        t0 = time.time()
        for path, img, im0s, vid_cap in dataset:
            img = torch.from_numpy(img).to(self.device)
            img = img.half() if self.half else img.float()  # uint8 to fp16/32
            img /= 255.0  # 0 - 255 to 0.0 - 1.0
            if img.ndimension() == 3:
                img = img.unsqueeze(0)

            # Warmup
            if self.device.type != 'cpu' and (old_img_b != img.shape[0] or old_img_h != img.shape[2] or old_img_w != img.shape[3]):
                old_img_b = img.shape[0]
                old_img_h = img.shape[2]
                old_img_w = img.shape[3]
                for i in range(3):
                    self.model(img, augment=opt.augment)[0]

            # Inference
            t1 = time_synchronized()
            with torch.no_grad():   # Calculating gradients would cause a GPU memory leak
                pred = self.model(img, augment=opt.augment)[0]
            t2 = time_synchronized()

            # Apply NMS
            pred = non_max_suppression(pred, opt.conf_thres, opt.iou_thres, classes=opt.classes, agnostic=opt.agnostic_nms)
            t3 = time_synchronized()

            print(pred)
            # Process detections
            for i, det in enumerate(pred):  # detections per image
                print(len(det))
                output_result = ''
                if self.webcam:  # batch_size >= 1
                    p, s, im0, frame = path[i], '%g: ' % i, im0s[i].copy(), dataset.count
                else:
                    p, s, im0, frame = path, '', im0s, getattr(dataset, 'frame', 0)

                p = Path(p)  # to Path
                save_path = str(self.save_dir / p.name)  # img.jpg
                txt_path = str(self.save_dir / 'labels' / p.stem) + ('' if dataset.mode == 'image' else f'_{frame}')  # img.txt
                gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh
                if len(det):
                    # Rescale boxes from img_size to im0 size
                    det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()

                    # Print results
                    for c in det[:, -1].unique():
                        n = (det[:, -1] == c).sum()  # detections per class
                        s += f"{n} {names[int(c)]}{'s' * (n > 1)}, "  # add to string
                        
                    def within_panel(xy, panel_xyxy):
                        """Check if xy is within panel_xyxy."""
                        if panel_xyxy is not None:  # Ensure panel_xyxy is not None before comparison
                            return panel_xyxy[0] <= xy[0] <= panel_xyxy[2] and panel_xyxy[1] <= xy[1] <= panel_xyxy[3]
                        return False  # If panel_xyxy is None, return False

                    ng = byPass = panel = runcard = False
                    bypass_cls = ['Blocked', 'Blurred', 'Empty', 'Dark']
                    ng_cls = ['Defect', 'Offset', 'Runcard']
                    det_bypass, det_ng = [0] * len(bypass_cls), [0] * len(ng_cls)
                    panel_data = [(xyxy, conf) for *xyxy, conf, cls in reversed(det) if names[int(cls)] == 'Panel']
                    panel_xyxy, _ = max(panel_data, key=lambda x: x[1], default=[None, None]) if panel_data else [None, None]
                    
                    for *xyxy, conf, cls in reversed(det):
                        cls_name = names[int(cls)]
                        label = f'{cls_name} {conf:.2f}'
                        
                        if cls_name in bypass_cls:
                            bidx = bypass_cls.index(cls_name)
                            byPass = det_bypass[bidx] = 1 
                            
                        elif cls_name == 'Panel' and panel_xyxy and all(a == b for a, b in zip(xyxy, panel_xyxy)):
                            panel = 1
                            
                        elif cls_name in ng_cls:
                            nidx = ng_cls.index(cls_name)
                            
                            if cls_name == 'Defect' and panel_xyxy is not None:
                                xy = [sum(xyxy[::2]) / 2, sum(xyxy[1::2]) / 2]
                                if within_panel(xy, panel_xyxy):
                                    ng = 1
                                    det_ng[nidx] += 1
                                    
                            elif cls_name == 'Offset':
                                ng = 1
                                det_ng[nidx] = 1
                                panel = False
                            elif cls_name == 'Runcard':
                                ng = 1
                                runcard = 1
                                det_ng[nidx] += 1
                        
                        if self.save_txt:
                            xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()
                            line = (cls, *xywh, conf) if opt.save_conf else (cls, *xywh)
                            with open(txt_path + '.txt', 'a') as f:
                                f.write(('%g ' * len(line)).rstrip() % line + '\n')

                        plot_one_box(xyxy, im0, label=label, color=colors[int(cls)], line_thickness=1)
                    
                    if runcard or not byPass:
                        if panel:
                            output_result += '1 Panel' + (', ' + ', '.join(f'{result} {cls}' for cls, result in zip(ng_cls, det_ng) if result) if any(det_ng) else '')
                        else:
                            output_result += ', '.join(f'{result} {cls}' for cls, result in zip(ng_cls, det_ng) if result)
                    else:
                        output_result += ' & '.join(cls for cls, result in zip(bypass_cls, det_bypass) if result)
						
                    cv2.putText(im0, output_result, (10, 710), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
                    
                    status = 'byPass' if byPass and not runcard else 'NG' if ng else 'PASS'
                    cv2.putText(im0, status, (10, 80), cv2.FONT_HERSHEY_SIMPLEX, 3, (0, 255, 255) if byPass and not runcard else (0, 0, 255) if ng else (0, 255, 0), 10)
                else:
                    byPass = 1
                    cv2.putText(im0, 'None', (10, 710), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
                    cv2.putText(im0, 'byPass', (10, 80), cv2.FONT_HERSHEY_SIMPLEX, 3, (0, 255, 255) if byPass else (0, 0, 255) if ng else (0, 255, 0), 10)
                # Print time (inference + NMS)
                print(f'{s}Done. ({(1E3 * (t2 - t1)):.1f}ms) Inference, ({(1E3 * (t3 - t2)):.1f}ms) NMS')
                return im0
            '''
                # Stream results
                if self.view_img:
                    cv2.imshow(str(p), im0)
                    cv2.waitKey(1)  # 1 millisecond

                # Save results (image with detections)
                if self.save_img:
                    if dataset.mode == 'image':
                        cv2.imwrite(save_path, im0)
                        print(f" The image with the result is saved in: {save_path}")
                    else:  # 'video' or 'stream'
                        if vid_path != save_path:  # new video
                            vid_path = save_path
                            if isinstance(vid_writer, cv2.VideoWriter):
                                vid_writer.release()  # release previous video writer
                            if vid_cap:  # video
                                fps = vid_cap.get(cv2.CAP_PROP_FPS)
                                w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                                h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                            else:  # stream
                                fps, w, h = 30, im0.shape[1], im0.shape[0]
                                save_path += '.mp4'
                            vid_writer = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))
                        vid_writer.write(im0)

        if self.save_txt or self.save_img:
            s = f"\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}" if self.save_txt else ''
            #print(f"Results saved to {save_dir}{s}")
        
        print(f'Done. ({time.time() - t0:.3f}s)')
        '''

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--weights', nargs='+', type=str, default='yolov7.pt', help='model.pt path(s)')
    parser.add_argument('--source', type=str, default='./test.jpg', help='source')  # file/folder, 0 for webcam
    parser.add_argument('--img-size', type=int, default=640, help='inference size (pixels)')
    parser.add_argument('--conf-thres', type=float, default=0.5, help='object confidence threshold')
    parser.add_argument('--iou-thres', type=float, default=0.45, help='IOU threshold for NMS')
    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')
    parser.add_argument('--view-img', action='store_true', help='display results')
    parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')
    parser.add_argument('--save-conf', action='store_true', help='save confidences in --save-txt labels')
    parser.add_argument('--nosave', action='store_true', help='do not save images/videos')
    parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --class 0, or --class 0 2 3')
    parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')
    parser.add_argument('--augment', action='store_true', help='augmented inference')
    parser.add_argument('--update', action='store_true', help='update all models')
    parser.add_argument('--project', default='runs/detect', help='save results to project/name')
    parser.add_argument('--name', default='exp', help='save results to project/name')
    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')
    parser.add_argument('--no-trace', action='store_true', help='don`t trace model')
    opt = parser.parse_args()
    print(opt)
    #check_requirements(exclude=('pycocotools', 'thop'))

    with torch.no_grad():
        if opt.update:  # update all models (to fix SourceChangeWarning)
            for opt.weights in ['yolov7.pt']:
                detect()
                strip_optimizer(opt.weights)
        else:
            d = detect()
            count = 0 
            path = './test/'
            while True:
                print('waiting images...')
                allFileList = os.listdir(path)
                for file in allFileList:
                    if file.endswith('.jpg') and 'result' not in file:
                        img = d.inference(path+file)
                        cv2.imwrite(path+file.replace('.jpg', '_result'+str(count)+'.jpg'), img)
                        os.remove(path+file)
                        count+=1
                time.sleep(3)

-----------------------------------------
Troubleshooting

如果訓練不小心中斷了，執行恢復訓練指令
python train.py --resume runs/train/yolov7-tiny-custom-M11/weights/last.pt

Tried to allocate xx MiB (GPU 0; xx GiB total capacity; xx GiB already allocated; xx MiB free; xx GiB reserved in total by PyTorch)…  [Ans: 記憶體不足，降低batch-size]

若因為某種原因，導致圖片與標記檔數量不匹配，但又找不出是哪幾個檔案，可執行程式imgtxt_compare.py刪除沒有成對的檔案

# -*- coding: utf-8 -*-
import os

# 資料夾路徑
folder_path = "./augmentation"

# 圖片檔案副檔名
image_extension = ".jpg"  # 根據你的圖片副檔名修改

# 標記檔案副檔名
label_extension = ".txt"  # 根據你的標記檔副檔名修改

# 列出所有圖片檔案
image_files = [f for f in os.listdir(folder_path) if f.endswith(image_extension)]

# 列出所有標記檔案
label_files = [f for f in os.listdir(folder_path) if f.endswith(label_extension)]

# 比對圖片與標記檔案的數量
for image_file in image_files:
    # 取得圖片的檔名（不包含副檔名）
    image_name = os.path.splitext(image_file)[0]
    
    # 檢查標記檔案是否存在
    if f"{image_name}{label_extension}" not in label_files:
        # 標記檔案不存在，刪除圖片
        os.remove(os.path.join(folder_path, image_file))
        print(f"刪除圖片: {image_file}")

for label_file in label_files:
    # 取得標記檔的檔名（不包含副檔名）
    label_name = os.path.splitext(label_file)[0]
    
    # 檢查圖片檔案是否存在
    if f"{label_name}{image_extension}" not in image_files:
        # 圖片檔案不存在，刪除標記檔
        os.remove(os.path.join(folder_path, label_file))
        print(f"刪除標記檔: {label_file}")

		
		
-------------------------------------------------------------------

For example

config.ini

[SETTINGS]
cam_idx = 0
cam_zoom = 150, 100, 0, 0, 0, 0, 0, 0
cpu_limit_percentage = 95
trigger_delay = 1
process_break = 1
test_mode = False
manual_mode = False

攝影機的索引值
從第一箱(最低)疊到第八箱(最高)攝影機要zoom in的數值
限制CPU使用率的百分比
拍照的延遲秒數
進程休息秒數(防止樹莓派過熱)
測試模式 : 循環讀取同一張圖進行判斷
手動模式 : 按F1鍵觸發一次拍照+判斷


detect_custom_raspberrypi.py

# coding: utf-8
import io
import os
import cv2
import sys
import time
import torch
import serial
import psutil
import shutil
import logging
import argparse
import datetime
import threading
import subprocess
import configparser
import pandas as pd
import torch.backends.cudnn as cudnn
import numpy as np
import multiprocessing as mp
from ftplib import FTP
from PLClib_new import PLC
from pathlib import Path
from pynput.keyboard import Key, Listener
from datetime import timedelta
from models.experimental import attempt_load
from utils.datasets import LoadStreams, LoadImages
from utils.general import check_img_size, check_requirements, check_imshow, non_max_suppression, apply_classifier, scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path
from utils.plots import plot_one_box
from utils.torch_utils import select_device, load_classifier, time_synchronized, TracedModel

HOST = "10.97.162.1"
POST= 5000
plc=PLC(HOST,POST)
save_og = True #儲存原圖

def job(devicePort, baud, timeout, XBee_MAC, data):
    ser = serial.Serial(devicePort, baud, timeout = timeout)
    dataStr = chr(0x02) + XBee_MAC;
    dataStr = dataStr + "," + str(len(data));
    for i in range(len(data)):
        dataStr = dataStr + "," + str(data[i]);
    dataStr = dataStr + chr(0x03) + chr(0x13) + chr(0x10) + '\r\n'
    portDataStr = bytes(dataStr, 'utf-8')
    ser.write(portDataStr)
    ser.close()    

class XBee():
    def __init__(self, XBee_MAC = 'XBee_MAC', devicePort = '/dev/ttyUSB0', baud = 9600, timeout=0.5, mode = 1):
        self.XBee_MAC = XBee_MAC
        self.devicePort = devicePort
        self.baud = baud
        self.timeout = timeout
        self.mode = mode
        self.bool_counter = 0
        
    def xbee_process(self, data):  #xbee_process(TEXT,XBee_MAC)
        if self.mode == 1:
            thread1 = mp.Process(target = job, args = (self.devicePort, self.baud, self.timeout, self.XBee_MAC, data, ))
            thread1.start()
        else:
            thread1 = threading.Thread(target = job, args = (self.devicePort, self.baud, self.timeout, self.XBee_MAC, data, ))
            thread1.start()
            
    def xbee_setPanID(self, panid):
        ser = serial.Serial(self.devicePort, self.baud, timeout = self.timeout)
        dataStr = "+++"
        portDataStr = bytes(dataStr, 'utf-8')
        ser.write(portDataStr)
        time.sleep(1)
        dataStr = '\r\n' + "ATID " + str(panid) + '\r\n' + "ATJV 1" + '\r\n' + "ATWR" + '\r\n' + "ATAC" + '\r\n' + "ATCN" + '\r\n'
        portDataStr = bytes(dataStr, 'utf-8')
        ser.write(portDataStr)
        time.sleep(1)
        ser.close()
        
    def xbee_result(self, result):
        if result == "PASS":
            print("PASS")
            if self.bool_counter == 0:
                self.xbee_process(['5'])
                self.bool_counter = 1
            else:
                self.xbee_process(['10'])
                self.bool_counter = 0
        elif result == "NG":
            print("NG")
            for i in range(5):
                self.xbee_process(['20'])
        else:
            self.xbee_process(['0'])

def delete_oldest_dirs_in_folder(path_to_scan = './NG_images'):
    # 取得指定資料夾中的目錄列表，並將目錄路徑和對應的日期時間對象存儲在list_of_dirs中
    list_of_dirs = [(os.path.join(path_to_scan, d), datetime.datetime.strptime(d, '%Y-%m-%d')) 
                    for d in os.listdir(path_to_scan) 
                    if os.path.isdir(os.path.join(path_to_scan, d))]
    list_of_dirs.sort(key=lambda item:item[1]) # 根據目錄的日期時間對象排序list_of_dirs，將最舊的目錄排在前面
    
    for dirpath, dirtime in list_of_dirs: # 迭代處理每個目錄
        try:
            shutil.rmtree(dirpath) # 刪除目錄
            logger.info(f"Removed directory: {dirpath}") # 記錄日誌消息，指示已刪除的目錄路徑
        except Exception as e:
            logger.warning(e) # 記錄刪除目錄時出現的異常作為警告消息
        
        hdd = psutil.disk_usage('/') # 檢查磁盤使用情況
        if hdd.percent < 90.0: # 如果磁盤使用百分比小於90.0%，停止進一步刪除目錄，跳出迴圈
            break
            
def space_check():
    hdd = psutil.disk_usage('/') # 檢查磁盤使用情況
    logger.info("Total space : "+  str(hdd.total // (2**30))+ "GB") # 記錄總空間大小 (以GB為單位)
    logger.info("Used space : "+  str(hdd.used // (2**30))+ "GB") # 記錄已使用空間大小 (以GB為單位)
    logger.info("Free space : "+  str(hdd.free // (2**30))+ "GB") # 記錄可用空間大小 (以GB為單位)
    logger.info("Percent used: "+  str(hdd.percent)+ "%") # 記錄磁盤使用百分比
    while hdd.percent >= 90.0: # 當磁盤使用百分比超過等於90.0%時，持續執行以下操作
        delete_oldest_dirs_in_folder() # 刪除最舊的目錄以釋放空間
        hdd = psutil.disk_usage('/') # 重新檢查磁盤使用情況
            
def clear_old_dirs():
    base_dir = './NG_images'  # 基礎資料夾路徑
    if not os.path.exists(base_dir): # 如果基礎資料夾不存在，則創建基礎資料夾
        os.mkdir(base_dir)
    if not os.path.exists('./logs'): # 如果日誌資料夾不存在，則創建日誌資料夾
        os.mkdir('./logs')
        
    dirs = os.listdir(base_dir) # 獲取基礎資料夾中的所有目錄列表
    for dir in dirs: # 迭代處理每個目錄
        dir_path = os.path.join(base_dir, dir)  # 目錄完整路徑
        if not os.path.isdir(dir_path): # 如果目錄不是一個有效的目錄，則跳過當前迴圈
            continue
            
        dir_date = datetime.datetime.strptime(dir, '%Y-%m-%d') # 將目錄名稱轉換為日期時間對象
        if dir_date < datetime.datetime.now() - timedelta(days=90): # 如果目錄日期早於90天前的日期，則刪除該目錄
            shutil.rmtree(dir_path)
    
def save_NG_image(img, og):
    current_date = datetime.date.today().strftime("%Y-%m-%d") # 獲取當前日期，並以'%Y-%m-%d'的格式轉換為字串
    date_dir_path = os.path.join('./NG_images', current_date) # 建立日期資料夾的完整路徑
    
    if not os.path.exists(date_dir_path): # 如果日期資料夾不存在，則創建日期資料夾
        os.mkdir(date_dir_path)
    
    time_str = datetime.datetime.now().strftime("%m%d_%H%M%S") # 獲取當前時間，並以'%m%d_%H%M%S'的格式轉換為字串
    img_path = os.path.join(date_dir_path, f'{time_str}_ng.jpg') # 儲存檢測結果圖像的檔案路徑
    cv2.imwrite(img_path, img) # 將檢測結果圖像寫入檔案
    
    if save_og: # 如果需要儲存原始圖像
        og_path = os.path.join(date_dir_path, f'{time_str}_og.jpg') # 儲存原始圖像的檔案路徑
        cv2.imwrite(og_path, og) # 將原始圖像寫入檔案
        
def save_image(img, og):
    current_date = datetime.date.today().strftime("%Y-%m-%d") # 獲取當前日期，並以'%Y-%m-%d'的格式轉換為字串
    date_dir_path = os.path.join('./All_images', current_date) # 建立日期資料夾的完整路徑
    
    if not os.path.exists(date_dir_path): # 如果日期資料夾不存在，則創建日期資料夾
        os.mkdir(date_dir_path)
    
    time_str = datetime.datetime.now().strftime("%m%d_%H%M%S") # 獲取當前時間，並以'%m%d_%H%M%S'的格式轉換為字串
    img_path = os.path.join(date_dir_path, f'{time_str}.jpg') # 儲存圖像的檔案路徑
    cv2.imwrite(img_path, img) # 將圖像寫入檔案
    
    if save_og: # 如果需要儲存原始圖像
        og_path = os.path.join(date_dir_path, f'{time_str}_og.jpg') # 儲存原始圖像的檔案路徑
        cv2.imwrite(og_path, og) # 將原始圖像寫入檔案

def is_file_open(file_path):
    try:
        with open(file_path, 'a'):
            return False
    except IOError:
        return True
    
def write_to_excel(data):
    # 格式化日期作为文件名
    file_name = time.strftime("%Y-%m-%d", time.localtime())
    if not os.path.exists('./excel'):
        os.makedirs('./excel')
    
    # 检查文件是否已被其他进程打开
    while is_file_open('./excel/'+f'{file_name}.xlsx'):
        # 如果文件被打开，则等待一段时间再检查
        time.sleep(1)
        
    # 将结果保存为 Excel 文件
    df = pd.DataFrame(data, columns=['日期', 'ID', '结果'])
    df.to_excel('./excel/'+f'{file_name}.xlsx', index=False)

def set_logger(current_date):
    log_filename = f"logs/M11_detection_{current_date}.log" # 設定日誌檔案名稱
    logger = logging.getLogger(__name__) # 建立新的日誌記錄器
    logger.setLevel(logging.DEBUG)
    
    if not logger.handlers: # 如果logger沒有已存在的處理器
        file_handler = logging.FileHandler(log_filename) # 建立檔案處理器並設定日誌格式
        file_handler.setFormatter(logging.Formatter('%(asctime)s %(levelname)s %(name)s %(message)s'))
        logger.addHandler(file_handler) # 新增檔案處理器到logger中
    else: # 如果logger已有已存在的處理器
        handler = logger.handlers[0] # 獲取第一個處理器
        
        if not current_date in handler.baseFilename: # 檢查處理器的檔案名稱是否包含當前日期
            logger.removeHandler(handler) # 移除已存在的處理器
            file_handler = logging.FileHandler(log_filename) # 建立新的檔案處理器並設定日誌格式
            file_handler.setFormatter(logging.Formatter('%(asctime)s %(levelname)s %(name)s %(message)s'))
            logger.addHandler(file_handler) # 新增新的檔案處理器到logger中
            
    return logger

def check_config(config):
    if not os.path.isfile('./config.ini'): # 如果config.ini檔案不存在
        config.add_section('SETTINGS') # 新增'SETTINGS'節到config中
        
        # 設定預設配置參數值到'SETTINGS'節中
        config.set('SETTINGS', 'cam_idx', '0') #相機索引
        room_list = ['150', '100', '0', '0', '0', '0', '0', '0'] #代表第一箱到第八箱要ROOM IN的數值(值要測)
        room_str = ', '.join(room_list)
        config.set('SETTINGS', 'cam_zoom', room_str)
        config.set('SETTINGS', 'cpu_limit_percentage', '95') #限制CPU使用率
        config.set('SETTINGS', 'trigger_delay', '1') #拍照延遲秒數
        config.set('SETTINGS', 'process_break', '1') #每一輪暫停秒數
        config.set('SETTINGS', 'test_mode', 'False') #測試模式:循環讀取同一張相片測試
        config.set('SETTINGS', 'manual_mode', 'False') #手動模式:按F1觸發拍照
        
        with open('config.ini', 'w') as configfile: # 開啟config.ini檔案並將配置參數寫入檔案
            config.write(configfile)

def trigger(cap, sec):
    time.sleep(sec) # 等待指定的秒數
    if cap.isOpened(): # 檢查攝像機是否已開啟
        ret, frame = cap.read() # 讀取攝像機的影格
        logger.info('* * * Taking Pic * * *') # 記錄拍攝圖片的日誌信息
        if ret == True: # 如果成功讀取到影格，則返回該影格
            return frame
        else:
            raise Exception('Trigger Failed') # 如果讀取影格失敗，則拋出異常


def load_camera(cam_idx):
    cap = cv2.VideoCapture(cam_idx) # 建立攝像機擷取物件
    if not cap.isOpened(): # 檢查攝像機是否成功開啟
        raise Exception('! ! ! Camera Load Failed ! ! !') # 如果無法開啟攝像機，拋出異常
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 960) # 設定攝像機的影像寬度為960像素
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720) # 設定攝像機的影像高度為720像素
    return cap

def upload_to_ftp(image):
    is_success, buffer = cv2.imencode(".jpg", image) # 將圖像編碼為JPEG格式
    data = io.BytesIO(buffer)
    try:
        ftp = FTP('10.97.124.95', timeout=1) # 建立FTP連接
        ftp.login(user='auo', passwd='1234') # 登入FTP伺服器
        ftp.cwd('/0808') # 切換到指定目錄
        ftp.storbinary('STOR test_upload.jpg', data) # 上傳二進位數據
        logger.info("Image uploaded successfully") # 記錄上傳成功的日誌信息
        ftp.quit() # 關閉FTP連接
        logger.info("FTP connection closed.") # 記錄FTP連接關閉的日誌信息
    except Exception as e:
        logger.warning(f"Failed to upload image. Reason: {str(e)}") # 記錄上傳失敗的警告信息
        #raise Exception('Failed to upload image')
        
class detect:
    def __init__(self):
        self.weights, self.view_img, self.save_txt, self.imgsz, self.trace = opt.weights, opt.view_img, opt.save_txt, opt.img_size, not opt.no_trace
        # 目錄
        self.save_dir = Path(increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok))  # 增加運行次數
        (self.save_dir / 'labels' if self.save_txt else self.save_dir).mkdir(parents=True, exist_ok=True)  # 建立目錄

        # 初始化
        set_logging()  # 設置日誌
        self.device = select_device(opt.device)  # 選擇設備
        self.half = self.device.type != 'cpu'  # 只支持CUDA上的半精度

        # 加載模型
        self.model = attempt_load(self.weights, map_location=self.device)  # 加載FP32模型
        self.stride = int(self.model.stride.max())  # 模型步長
        self.imgsz = check_img_size(self.imgsz, s=self.stride)  # 檢查圖像尺寸

        if self.trace:
            self.model = TracedModel(self.model, self.device, opt.img_size)

        if self.half:
            self.model.half()  # 轉換為FP16
            
        self.Run_Count = 0 #紀錄總執行次數
        self.Pass_Count = 0 #無異常的次數
        self.NG_Count = 0 #異常次數
        self.ByPass_Count = 0 #不檢測次數
        
        #self.a = XBee(devicePort = "/dev/ttyUSB0", mode = 0)
        #self.a.xbee_setPanID(168)
            
    def inference(self, image):
        global excel_result
        excel_date = time.strftime("%Y/%m/%d %H:%M", time.localtime())
        self.Run_Count+=1 # 執行次數 +1
        self.source = image
        dataset = LoadImages(self.source, img_size=self.imgsz, stride=self.stride)
        # Get names and colors
        names = self.model.module.names if hasattr(self.model, 'module') else self.model.names
        colors = [[np.random.randint(0, 255) for _ in range(3)] for _ in names]

        # Run inference
        if self.device.type != 'cpu': # 預熱模型
            self.model(torch.zeros(1, 3, self.imgsz, self.imgsz).to(self.device).type_as(next(self.model.parameters())))  # run once
        old_img_w = old_img_h = self.imgsz
        old_img_b = 1

        t0 = time.time() # 開始時間
        for path, img, im0s, vid_cap in dataset:
            img = torch.from_numpy(img).to(self.device)
            img = img.half() if self.half else img.float()  # uint8 to fp16/32
            img /= 255.0  # 0 - 255 to 0.0 - 1.0
            if img.ndimension() == 3:
                img = img.unsqueeze(0)

            # Warmup
            if self.device.type != 'cpu' and (old_img_b != img.shape[0] or old_img_h != img.shape[2] or old_img_w != img.shape[3]):
                old_img_b = img.shape[0]
                old_img_h = img.shape[2]
                old_img_w = img.shape[3]
                for i in range(3):
                    self.model(img, augment=opt.augment)[0]

            og_img = im0s.copy()
            # Inference
            t1 = time_synchronized()
            with torch.no_grad():   # Calculating gradients would cause a GPU memory leak
                pred = self.model(img, augment=opt.augment)[0]
            t2 = time_synchronized()

            # Apply NMS
            pred = non_max_suppression(pred, opt.conf_thres, opt.iou_thres, classes=opt.classes, agnostic=opt.agnostic_nms)
            t3 = time_synchronized()
            
            # Process detections
            for i, det in enumerate(pred):  # detections per image
                p, s, im0, frame = path, '', im0s, getattr(dataset, 'frame', 0)
                output_result = ''
                if len(det):
                    # Rescale boxes from img_size to im0 size
                    det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()

                    # Print results
                    for c in det[:, -1].unique():
                        n = (det[:, -1] == c).sum()  # detections per class
                        s += f"{n} {names[int(c)]}{'s' * (n > 1)}, "  # add to string
                        
                    def within_panel(xy, panel_xyxy):
                        """Check if xy is within panel_xyxy."""
                        if panel_xyxy is not None:  # Ensure panel_xyxy is not None before comparison
                            return panel_xyxy[0] <= xy[0] <= panel_xyxy[2] and panel_xyxy[1] <= xy[1] <= panel_xyxy[3]
                        return False  # If panel_xyxy is None, return False

                    ng = byPass = panel = False
                    bypass_cls = ['Blocked', 'Blurred', 'Empty', 'Dark'] #ByPass的類別
                    ng_cls = ['Defect', 'Offset', 'Runcard'] #NG的類別
                    det_bypass, det_ng = [0] * len(bypass_cls), [0] * len(ng_cls) #宣告陣列來紀錄類別的有無
                    panel_data = [(xyxy, conf) for *xyxy, conf, cls in reversed(det) if names[int(cls)] == 'Panel'] #儲存Panel的座標和信心度
                    panel_xyxy, _ = max(panel_data, key=lambda x: x[1], default=[None, None]) if panel_data else [None, None] #找出最大信心度的Panel座標
                    
                    for *xyxy, conf, cls in reversed(det): #循環檢測結果
                        cls_name = names[int(cls)] #類別名稱
                        label = f'{cls_name} {conf:.2f}'
                        
                        if cls_name in bypass_cls: #如果類別為ByPass的一種
                            bidx = bypass_cls.index(cls_name)
                            byPass = det_bypass[bidx] = 1 
                            
                        elif cls_name == 'Panel' and panel_xyxy and all(a == b for a, b in zip(xyxy, panel_xyxy)): #如果類別為Panel
                            panel = 1
                            
                        elif cls_name in ng_cls: #如果類別為NG的一種
                            nidx = ng_cls.index(cls_name)
                            
                            if cls_name == 'Defect' and panel_xyxy is not None: #有異物且有Panel
                                xy = [sum(xyxy[::2]) / 2, sum(xyxy[1::2]) / 2]
                                if within_panel(xy, panel_xyxy):
                                    ng = 1
                                    det_ng[nidx] += 1
                                    
                            elif cls_name == 'Offset': #偏移
                                ng = 1
                                det_ng[nidx] = 1
                                panel = False
                            elif cls_name == 'Runcard' and conf.item()>0.89: #Runcard未撕
                                ng = 1
                                det_ng[nidx] += 1
                                
                        if self.save_txt:
                            xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()
                            line = (cls, *xywh, conf) if opt.save_conf else (cls, *xywh)
                            with open(txt_path + '.txt', 'a') as f:
                                f.write(('%g ' * len(line)).rstrip() % line + '\n')
                            
                        plot_one_box(xyxy, im0, label=label, color=colors[int(cls)], line_thickness=1) #圖上畫框
                    
                    if not byPass: #要輸出的結果
                        if panel:
                            output_result += '1 Panel' + (', ' + ', '.join(f'{result} {cls}' for cls, result in zip(ng_cls, det_ng) if result) if any(det_ng) else '')
                        else:
                            output_result += ', '.join(f'{result} {cls}' for cls, result in zip(ng_cls, det_ng) if result)
                    else:
                        output_result += ' & '.join(cls for cls, result in zip(bypass_cls, det_bypass) if result)
                        self.ByPass_Count+=1

                    cv2.putText(im0, output_result, (10, im0.shape[0]), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2) #印類別在圖上
                    
                    status = 'byPass' if byPass else 'NG' if ng else 'PASS'
                    cv2.putText(im0, status, (10, 80), cv2.FONT_HERSHEY_SIMPLEX, 3, (0, 255, 255) if byPass else (0, 0, 255) if ng else (0, 255, 0), 10) #印結果在圖上
                    
                    excel_id = "SuperPIE"
                    excel_result.append((excel_date, excel_id, status))
                    write_to_excel(excel_result)
                    
                    try:
                        self.a.xbee_result(status)
                    except Exception as e:
                        print('XBEE Error')
                    
                    if ng and not byPass: #存NG圖和原圖
                        #port2_val = plc.get('D',4603,1)
                        plc.set('D', 4550,1,[2])
                        upload_thread = threading.Thread(target=upload_to_ftp, args=(im0,))
                        save_thread = threading.Thread(target=save_NG_image, args=(im0,og_img,))
                        upload_thread.start()
                        save_thread.start()
                        self.NG_Count+=1
                    elif not byPass:
                        self.Pass_Count+=1
                else:
                    byPass = 1
                    self.ByPass_Count+=1
                    cv2.putText(im0, 'None', (10, im0.shape[0]), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
                    cv2.putText(im0, 'byPass', (10, 80), cv2.FONT_HERSHEY_SIMPLEX, 3, (0, 255, 255) if byPass else (0, 0, 255) if ng else (0, 255, 0), 10)
                    
            #Save the full image to test the results
            #save_all = threading.Thread(target=save_image, args=(im0,og_img,))
            #save_all.start()
            
            logger.info('Inference Result : '+ output_result)
            logger.info('Inference Time: ' + f'{(1E3 * (t2 - t1)):.1f}ms' + '/ NMS Time: ' + f'{(1E3 * (t3 - t2)):.1f}ms')

            pass_percent = (self.Pass_Count / self.Run_Count) * 100
            ng_percent = (self.NG_Count / self.Run_Count) * 100
            bypass_percent = (self.ByPass_Count / self.Run_Count) * 100

            logger.info(f'Pass : {self.Pass_Count}, percentage: {pass_percent:.2f}%')
            logger.info(f'NG : {self.NG_Count}, percentage: {ng_percent:.2f}%')
            logger.info(f'ByPass : {self.ByPass_Count}, percentage: {bypass_percent:.2f}%')
            logger.info(f'Total count : {self.Run_Count}')
            #return im0
        
def get_cpu_temp():
    """獲取CPU溫度。"""
    return round(psutil.sensors_temperatures()['cpu_thermal'][0].current, 1)

def get_cpu_used():
    """獲取CPU使用率。"""
    return psutil.cpu_percent(interval=None)

def get_gpu_temp():
    """獲取GPU溫度。"""
    gpu_temp_info = os.popen('vcgencmd measure_temp').readline()
    return float(gpu_temp_info.replace("temp=","").replace("'C\n",""))
'''
def limit_cpu_usage():
    pid = os.getpid()  # 獲取當前進程的ID
    command = f"cpulimit -l 90 -p {pid} -z"  # 指定CPU限制命令
    subprocess.Popen(command, preexec_fn=os.setsid, shell=True)  # 創建子進程，執行命令
    return pid  # 返回當前進程的ID
'''
def safety_check():
    cpu_used = get_cpu_used()  # 獲取CPU使用率
    while cpu_used > cpu_limit_percentage:
        time.sleep(0.1)  # 等待0.1秒

    cpu_temp = get_cpu_temp()  # 獲取CPU溫度
    gpu_temp = get_gpu_temp()  # 獲取GPU溫度

    logger.info('CPU Usage: '+ str(cpu_used) + '%, ' + 'CPU Temp: ' + str(cpu_temp) + '°C / ' + 'GPU Temp: ' + str(gpu_temp) + '°C')

    while cpu_temp > 75 or gpu_temp > 75:
        logger.warning(f"Stopped due to high temperature. CPU Temp: {cpu_temp} | GPU Temp: {gpu_temp}")
        logger.warning("Sleeping for 10 minutes for cooling down.")  # 高溫警告，休眠10分鐘進行降溫
        time.sleep(600)  # 休眠10分鐘
        cpu_temp = get_cpu_temp()  # 重新獲取CPU溫度
        gpu_temp = get_gpu_temp()  # 重新獲取GPU溫度
        if cpu_temp < 65 or gpu_temp < 65:
            logger.info("Successfully Cooling down . Return to work!!!")  # 降溫成功，返回正常工作
            break
            
if __name__ == '__main__':
    parser = argparse.ArgumentParser()  # 建立解析器
    parser.add_argument('--weights', nargs='+', type=str, default='./v7_tiny.pt', help='model.pt path(s)')  # 加入參數weights，預設值為'./v7_tiny.pt'
    parser.add_argument('--cam-idx', type=int, default=0, help='Camera index')  # 加入參數cam-idx，預設值為0
    parser.add_argument('--cpu-limit-percentage', type=int, default=95, help='CPU usage limited')  # 加入參數cpu-limit，預設值為95
    parser.add_argument('--trigger-delay', type=int, default=1, help='Trigger taking photo delay time')  # 加入參數trigger-delay，預設值為1
    parser.add_argument('--process-break', type=int, default=5, help='Every process cycle break time')  # 加入參數process-break，預設值為5
    parser.add_argument('--source', type=str, default='./test.jpg', help='source')  # 加入參數source，預設值為'./test.jpg'
    parser.add_argument('--img-size', type=int, default=640, help='inference size (pixels)')  # 加入參數img-size，預設值為640
    parser.add_argument('--conf-thres', type=float, default=0.5, help='object confidence threshold')  # 加入參數conf-thres，預設值為0.5
    parser.add_argument('--iou-thres', type=float, default=0.45, help='IOU threshold for NMS')  # 加入參數iou-thres，預設值為0.45
    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')  # 加入參數device，預設值為''
    parser.add_argument('--load-config', action='store_false', help='create and load config.ini file')  # 加入參數load-config，預設值為False
    parser.add_argument('--test-mode', action='store_true', help='Run test image')  # 加入參數test-mode，預設值為True
    parser.add_argument('--manual-mode', action='store_true', help='Manual trigger')  # 加入參數manual-mode，預設值為True
    parser.add_argument('--limit-cpu', action='store_true', help='limit cpu usage')  # 加入參數limit-cpu，預設值為True
    parser.add_argument('--view-img', action='store_true', help='display results')  # 加入參數view-img，預設值為True
    parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')  # 加入參數save-txt，預設值為True
    parser.add_argument('--save-conf', action='store_true', help='save confidences in --save-txt labels')  # 加入參數save-conf，預設值為True
    parser.add_argument('--nosave', action='store_true', help='do not save images/videos')  # 加入參數nosave，預設值為True
    parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --class 0, or --class 0 2 3')  # 加入參數classes，為int型態，用於指定類別篩選
    parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')  # 加入參數agnostic-nms，預設值為True，用於進行類別-不可知NMS
    parser.add_argument('--augment', action='store_true', help='augmented inference')  # 加入參數augment，預設值為True，用於進行增強推理
    parser.add_argument('--update', action='store_true', help='update all models')  # 加入參數update，預設值為True，用於更新所有模型
    parser.add_argument('--project', default='runs/detect', help='save results to project/name')  # 加入參數project，預設值為'runs/detect'
    parser.add_argument('--name', default='exp', help='save results to project/name')  # 加入參數name，預設值為'exp'
    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')  # 加入參數exist-ok，預設值為True，指定已存在的project/name是可以接受的，不需要增加
    parser.add_argument('--no-trace', action='store_false', help='don`t trace model')  # 加入參數no-trace，預設值為False，指定不跟蹤模型
    opt = parser.parse_args()  # 解析參數並存儲到opt變數中
    current_date = datetime.date.today().strftime("%Y-%m-%d")  # 取得當前日期
    last_check_time = datetime.datetime.now()  # 取得當前時間
    logger = set_logger(current_date)  # 設定日誌文件
    excel_result = []
    clear_old_dirs()  # 清除舊的目錄
    space_check()  # 檢查磁碟空間

    if opt.load_config:
        config = configparser.ConfigParser()  # 建立設定檔解析器
        check_config(config)  # 檢查設定檔是否存在並創建
        config.read('./config.ini')  # 讀取設定檔
        cam_idx = config.getint('SETTINGS', 'cam_idx')  # 從設定檔獲取相機索引
        room_str = config.get('SETTINGS', 'cam_zoom')  # 從設定檔獲取相機縮放設定
        room_list = room_str.split(', ')  # 將相機縮放設定轉換成列表形式
        cpu_limit_percentage = config.getint('SETTINGS', 'cpu_limit_percentage')  # 從設定檔獲取CPU使用限制
        trigger_delay = config.getint('SETTINGS', 'trigger_delay')  # 從設定檔獲取觸發拍照延遲時間
        process_break = config.getint('SETTINGS', 'process_break')  # 從設定檔獲取進程中斷時間
        test_mode = config.getboolean('SETTINGS', 'test_mode', fallback=False)  # 從設定檔獲取測試模式
        manual_mode = config.getboolean('SETTINGS', 'manual_mode', fallback=False)  # 從設定檔獲取手動模式
    else:
        cam_idx = opt.cam_idx  # 從命令行參數中獲取相機索引
        room_list = [0, 0, 0, 0, 0, 0, 0, 0]  # 相機縮放設定列表
        cpu_limit_percentage = opt.cpu_limit_percentage  # 從命令行參數中獲取CPU使用限制百分比
        #limit_cpu = opt.limit_cpu # 從命令行參數中獲取使否限制CPU使用
        trigger_delay = opt.trigger_delay  # 從命令行參數中獲取觸發拍照延遲時間
        process_break = opt.process_break  # 從命令行參數中獲取進程中斷時間
        test_mode = opt.test_mode  # 從命令行參數中獲取測試模式
        manual_mode = opt.manual_mode  # 從命令行參數中獲取手動模式

    with torch.no_grad():
        if opt.update:  # update all models (to fix SourceChangeWarning)
            for opt.weights in ['yolov7.pt']:
                detect()
                strip_optimizer(opt.weights)
        else:
            try:
                d = detect() # 載入模型
                logger.info('* * * Model Loaded Successfully * * *')
            except Exception as e: # 模型載入失敗
                logger.critical('! ! ! Model Load Failed ! ! !')
                logger.critical(e)
                sys.exit('! ! ! Model Load Failed ! ! !')
            '''
            if limit_cpu: # 限制 CPU 使用率
                pid = limit_cpu_usage()
                logger.info('* * * CPU Usage Limited * * *')
            '''
            time.sleep(1)
            if not test_mode:
                try:
                    cap = load_camera(cam_idx) # 載入相機
                    logger.info('* * * Camera Loaded Successfully * * *')
                    if manual_mode: # 手動模式
                        logger.info('* * * * * * Manual Mode * * * * * *')
                        logger.info('--------------------------------------------------')
                        def on_press(key):
                            if key == Key.f1: # 觸發拍攝並進行推論
                                frame = trigger(cap, trigger_delay)
                                logger.info('- - - - - Manual Run Once - - - - -')
                                d.inference(frame)
                                safety_check()
                                logger.info('--------------------------------------------------')
                        listener = Listener(on_press=on_press)
                        listener.start()
                    else: # 自動模式
                        logger.info('* * * * * * Auto Mode * * * * * *')
                except Exception as e: # 相機載入失敗
                    logger.critical('! ! ! Camera Load Failed ! ! !')
                    logger.critical(e)
                    sys.exit('! ! ! Camera Load Failed ! ! !')
            else: # 測試模式
                source = opt.source
                manual_mode = False
                logger.info('* * * * * * Test Mode * * * * * *')
            
            start_time = time.time()
            try:
                while not manual_mode:
                    logger.info('--------------------------------------------------')
                    if not test_mode:
                        try:
                            box_idx = 7
                            plc_value = plc.get('D', 4603, 1) # 讀取 PLC 值
                            plc_value = int(plc_value.item())
                            print('PLC VALUE / Boxes num : '+ str(plc_value))
                            box_idx = plc_value if isinstance(plc_value, int) and 0 <= plc_value <= 8 else 7
                        except Exception as e:
                            print(e)
                            pass
                        cap.set(cv2.CAP_PROP_ZOOM, int(room_list[box_idx-1])) # 設定相機縮放值
                        print('Cam Zoom in Value : ' + str(room_list[box_idx-1]))
                        try: # 觸發拍攝並取得影像
                            source = trigger(cap, process_break)
                        except Exception as e:
                            logger.critical('! ! ! Capture Image Failed ! ! !')
                            logger.critical(e)
                            sys.exit('! ! ! Capture Image Failed ! ! !')
                    else:
                        time.sleep(process_break)
                    
                    d.inference(source)
                    safety_check()
                        
                    elapsed_time = time.time() - start_time
                    elapsed_hms = str(timedelta(seconds=elapsed_time))
                    logger.info(f"Total running time: {elapsed_hms} (hour:min:sec)")
                        
                    if datetime.date.today().strftime("%Y-%m-%d") != current_date: # 如果日期已經變更，重新設定 logger
                        current_date = datetime.date.today().strftime("%Y-%m-%d")
                        logger = set_logger(current_date)
                        excel_result = []
                        
                    if datetime.datetime.now() - last_check_time > timedelta(hours=1): # 每隔一小時檢查清除舊的資料夾和檢查硬碟空間
                        clear_old_dirs()
                        space_check()
                        last_check_time  = datetime.datetime.now()
                        
                while manual_mode:
                    time.sleep(1)
            except KeyboardInterrupt:
                sys.exit('! ! ! User Stopped ! ! !')
                
            finally:
                if not test_mode: # 釋放相機資源並關閉視窗
                    cap.release()
                    cv2.destroyAllWindows()
                    if manual_mode:
                        listener.stop()
                #if limit_cpu:
                    #proc.terminate()




